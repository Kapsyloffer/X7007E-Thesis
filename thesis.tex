\documentclass[12pt,a4paper]{report}

% Packages
\usepackage[utf8]{inputenc}
\usepackage{setspace}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{amsmath} 
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{float}

\usepackage{tikz}
\usetikzlibrary{arrows.meta, calc, positioning, patterns, fadings}

\tikzstyle{block} = [draw, fill=white, rectangle, 
    minimum height=3em, minimum width=6em]
\tikzstyle{sum} = [draw, fill=white, circle, node distance=1cm]
\tikzstyle{input} = [coordinate]
\tikzstyle{output} = [coordinate]
\tikzstyle{pinstyle} = [pin edge={to-,thin,black}]

\onehalfspacing
\geometry{margin=1in}

\titleformat{\chapter}[hang]
  {\normalfont\huge\bfseries}
  {\thechapter.}
  {1em}
  {}

\begin{document}

\title{Applying Deep Learning on Sequential Data within the Mixed-Model Assembly Line problem}
\author{Christoffer Lindkvist}
\date{\today}
\maketitle

\begin{abstract} 
    A mixed-model assembly line manufactures different product variants on a single line, where variations in tasks can create imbalances across the workstations. When several labour-intensive models appear consecutively, some stations may exceed their capacity, leading to overloads which requires halting the entire assemblyline to remedy the issues. This challenge is formalized as \textit{the Product Sequencing Problem}, an NP-hard optimization task concerned with arranging production orders into efficient sequences. This thesis investigates whether deep learning can complement heuristic methods for solving this problem. Using a Transformer-, Pointer Network-, and Sequence to Sequence model trained to emulate tacit scheduling knowledge captured in historical data, and using its predictions to initialize a heuristic algorithm. By providing informed starting points, this approach aims to reduce the computation time of the algorithm by presenting an adequate starting point.
\end{abstract}

\chapter*{Acknowledgements} 
Funding: Vinnova grant 2023-00970 (EUREKA ITEA4 ArtWork), Vinnova grant 2023-00450 (Arrowhead fPVN, Swedish
funding), and KDT JU grant 2023-000450 (Arrowhead fPVN,
EU funding).


\tableofcontents

\chapter{Introduction}
\section{Background}
This thesis addresses machine learning applied to the Product Sequencing Problem, an NP-Hard optimization problem which arises in the planning of mixed-model assembly lines \cite{ref6}. Traditionally, product sequences are determined manually by management staff, relying primarily on tacit knowledge accumulated through experience. While this often produces feasible solutions with relatively few scheduling conflicts, it remains ad hoc and sporadic. %ad hoc = academic for "doohickey"

To improve upon this, a heuristic algorithm has been proposed. The algorithm first generates a feasible baseline solution from pre-defined constraints, and then it refines the baseline until an acceptable sequence is reached. The central question of this thesis is whether the runtime of such an algorithm can be reduced by initializing it with a baseline informed by historical sequencing data, rather than relying solely on rule-based object placements. The baseline in question will be generated using a deep-learning model. 

\section{Research Problem}
Current approaches in practice rely entirely on human expertise and tacit knowledge, which limits scalability and consistency. If this knowledge could be systematically emulated using a model that mimics historical sequencing data, and in effect tacit knowledge, it may provide stronger starting points for any given heuristic method. Such informed baselines could potentially reduce the runtime required to reach high-quality solutions, especially for complex sequencing instances. 

\clearpage %TODO: Ta bort om nödvändigt
\section{Objectives}
The objectives of this thesis are:
\begin{enumerate}
    \item To design a deep learning model capable of emulating the tacit knowledge of management workers using historical data.
    \item To investigate which model performs the best in the given task of permuting input data to an acceptable solution.
    \item To integrate the model as a preprocessing step in the existing heuristic algorithm in order to reduce its runtime.
    \item To provide a visualization tool that intuitively illustrates the scheduling flow, highlighting overlaps, borrowed time, and bottlenecks across stations and time.
\end{enumerate}

\section{Visualization of the Problem}

The user interface (UI) will visualize the flow of the assembly line along two axes: one representing stations and one representing clockcycles. A clockcycle is defined as the time required for an item to move from one station to the next. In the visualization, items are displayed in a way that reflects the relative duration of processing at each station. In \autoref{fig:assembly}, this is illustrated by stretching items along the timeline to better represent the clockcycles. Note that a clockcycle is an arbitrary unit of time and does not correspond to real-world durations. For the purpose of this thesis, one clockcycle is defined as the time it takes for an arbitrary item $X$ to move from station $S_n$ to station $S_{n+1}$. 

Each entry, whose size represents the time it needs to complete its cycle, may borrow time from a previous or upcoming station, this is refered to as the \textit{"drift area"}. Unfortunately, this is where the problems related to the Mixed-Model Assembly Lines start to arise. If time-intensive items are placed consecutively, we will experience an overlap, as the time-allocations will not fit given the constraints of the station and drift areas.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[scale=1, every node/.style={scale=0.8, inner sep=3pt}]

        \foreach \y in {-1,-3,-5,-7,-9, -11} {
            \fill[gray!20] (0,\y+0.5) rectangle (10,\y);
        }

        \draw[->] (0,0) -- (10,0) node[right]{Time};
        \draw[->] (0,0) -- (0,-12) node[below]{Stations};

        \foreach \y/\name in { -2/$S_1$, -4/$S_2$, -6/$S_3$, -8/$S_4$, -10/$S_5$ } {
            \node[left] at (0,\y) { \name };
        }
        \draw[fill=white] (0.5,-9.5) rectangle (1.5,-10.5) node[midway]{I};

        \draw[fill=white] (0.5,-7) rectangle (1.5,-9) node[midway]{H};
        \draw[fill=white] (2.5,-9.25) rectangle (3.5,-10.25) node[midway]{H};

        \draw[fill=white] (0.5,-5) rectangle (1.5,-6.5) node[midway]{G};
        \draw[fill=white] (2.5,-6.5) rectangle (3.5,-8) node[midway]{G};
        \draw[fill=white] (4.5,-8.5) rectangle (5.5,-11) node[midway]{G};

        \draw[fill=white] (0.5,-3) rectangle (1.5,-4.5) node[midway]{F};
        \draw[fill=white] (2.5,-5) rectangle (3.5,-6) node[midway]{F};
        \draw[fill=white] (4.5,-6.5) rectangle (5.5,-9) node[midway]{F};
        \draw[fill=white] (6.5,-9.25) rectangle (7.5,-10.5) node[midway]{F};

        \draw[fill=white] (0.5,-0.5) rectangle (1.5,-2.5) node[midway]{E};
        \draw[fill=white] (2.5,-2.5) rectangle (3.5,-4) node[midway]{E};
        \draw[fill=white] (4.5,-5) rectangle (5.5,-6.5) node[midway]{E};
        \draw[fill=white] (6.5,-7) rectangle (7.5,-8) node[midway]{E};
        \draw[fill=white] (8.5,-8.5) rectangle (9.5,-11) node[midway]{E};

        \draw[fill=white] (2.5,-0.5) rectangle (3.5,-2) node[midway]{D};
        \draw[fill=white] (4.5,-2.5) rectangle (5.5,-4.5) node[midway]{D};
        \draw[fill=white] (6.5,-4.5) rectangle (7.5,-6.5) node[midway]{D};
        \draw[fill=white] (8.5,-6.5) rectangle (9.5,-8.5) node[midway]{D};

        \draw[fill=white] (4.5,-0.5) rectangle (5.5,-2) node[midway]{C};
        \draw[fill=white] (6.5,-2.5) rectangle (7.5,-4.5) node[midway]{C};
        \draw[fill=white] (8.5,-5.25) rectangle (9.5,-6.25) node[midway]{C};

        \draw[fill=white] (6.5,-0.5) rectangle (7.5,-2) node[midway]{B};
        \draw[fill=white] (8.5,-2.5) rectangle (9.5,-4.25) node[midway]{B};

        \draw[fill=white] (8.5,-0.5) rectangle (9.5,-1.75) node[midway]{A};

\draw[pattern=north east lines, pattern color=black] (4.5,-8.5) rectangle (5.5,-9) node[midway]{};
        
\draw[pattern=north east lines, pattern color=black] (8.5,-8.5) rectangle (9.5,-9) node[midway]{};

        \foreach \x/\label in {2/T-2, 4/T-1, 6/T, 8/T+1, 10/T+2} {
            \draw[dashed] (\x,0) -- (\x,-12);
            \node[above] at (\x-1,0.2) {\label};
        }

        \draw (10.3,-0.5) -- (10.3,-1) node[midway,right]{|---| Drift Area };
        \draw (10.3,-1.1) -- (10.3,-2.4) node[midway,right]{|---| Station };

        \draw (10.3,-8.5) -- (10.3,-9) node[midway,right]{|---| Overlaps between $S_4$ and $S_5$};


    \end{tikzpicture}
    \caption{Assembly Line Example with Uniform Station and clockcycles}
    \label{fig:assembly}
\end{figure}

    Issues in visualizing this way start to appearing when we start to consider that different stations $S_n$ and $S_m$ may take different times to complete. If we then step a clockcycle for each possible item, then we can never keep our items in sync. The main issue is that; if we compare the station $S_n$ and $S_m$, then we'll see that each station have a different time to finish, then the clockcycle system will not be perfect or even realistic as stations with differing times will each finish in different times and thus an item $X$ might make it to the station $S_{n+2}$ from $S_n$ in the same time it takes item $Y$ to make it to $S_{m+1}$ from $S_{m}$.

Thus we find the difficulty in displaying it properly in an intuitive graphical user interface. If we wish to display each station as uniform sizes, then we also have to stretch the items to make up for it visually. But doing this we have no intuitive way of knowing that $S_4$ could be 200 seconds long in real life, while $S_3$ could be 300. \textit{As luck would have it, each station in this specific case are each roughly 7 minutes long, 700cmin}, thus we will not run into any major desync problems using clockcycles on these stations.

Between each station lies a buffer zone refered to a "drift area". A drift area in this case is a transitional area between any given station $S_n$ and $S_{n+1}$. Both of the stations can borrow time from each other within this area, but only one station may utilize that area at the time. This proves useful to help fit items that take longer on some stations onto the assembly line, but these conveniences which at first glance makes this problem easier are also the source of problems that may ensue in production.

As pictured in \autoref{fig:assembly}, $D$ will take a lot of time on $S_4$ and is forced to utilize time from $S_3$ and $S_5$. 
While this works well in a vacuum, the problems start to arise when $E$ also has to utilize additional time from its neighbouring stations, causing an overlap between $D$ and $E$ at $T+2$ as they both require the use of the drift area. 

The same problem can be seen at $T$ with $F$ and $G$ as both items need to borrow time from the stations before and after. Thus we run into an overlap, as the items \textit{cannot} fit.

Do note that on $T$, $E$ does not utilize the drift area which results in it sitting flush with $F$ on the timeline, this may look good on paper but can result in overlap in practice due to the human workers at the assembly line occasionally taking a bit longer than presumed. This can be resolved by borrowing some time from $S_2$ and moving $E$ into the drift area. 

The same issue derives at $T+1$ where $C$ and $D$ just \textit{barely} get enough time, but it cannot get resolved by simply moving $D$ forward, as $D$ on $T+2$ will require all of the time it can get on $S_4$. 

\section{Jump-in and Jump-out}
In modern production, product sequences are typically finalized and frozen in batches before entering the assembly line. Ideally, the order remains unchanged, but last-minute adjustments may occasionally be necessary. When a product is missing critical components and cannot be built, it cannot proceed on the assembly line. In such cases, the product must be temporarily removed from the sequence and held until all parts are available, a process referred to as \emph{jump-out}.

A related challenge occurs when the missing components finally arrive. At this point, the product occupies space on the assembly station. Ideally, it should be reintegrated into the line as soon as possible, preferably before the next batch begins production. Currently, however, reintegration is delayed, and the suspended product may not re-enter the assembly line until several batches later. \cite{ref6}

From an algorithmic perspective, a \emph{jump-in} can be treated as an $O(n)$ problem: one needs only to inspect the $n$ positions in a sequence of length $n$ to determine the best insertion point in the upcoming batch. Implemented correctly, a jump-in could occur as soon as the following batch. Nevertheless, depending on operational constraints, it may not be feasible in every batch.

Jump-out, on the other hand, can be considered an $O(1)$ problem, though it presents additional practical challenges. Removing a product from the assembly line creates a gap that must be managed. This gap may require shifting a portion of the line by a clock cycle, potentially causing overloads. Alternatively, the gap could be left in place, which avoids overloading but may result in lost revenue.

\clearpage %blir fult annars
\section{Defining Overlaps}
\label{chap:overlap}

Overlaps in this case arise from timing dependencies: each item’s progress depends on how long it takes to process at its respective station. If two items are processed consecutively at station $s_n$, and both of those items require additional time from its neighbouring stations $s_{n-1}$ and $s_{n+1}$ there will not be sufficient time for all operations to complete. This results in what's visualized as an overlap. In practice the items do not literally stack or collide. Rather, the assembly line must halt in order to allow these operations to complete before production may continue.


\begin{figure}[htbp]
\centering

\begin{minipage}{0.45\textwidth}
\centering
\begin{tikzpicture}[>=Latex, font=\large, scale=0.5, transform shape]

  \foreach \x in {0, 6, 2*6} {
    \fill[gray!20] (\x - 1.2/2, -0.6) rectangle (\x + 1.2/2, 1.5 + 0.6);
  }

  \draw[thick, fill=white] (-0.6, 0) rectangle (6 - 1.2/2+1.2, 1.5);
  
  \draw[thick, fill=white] (6 - 1.2/2+1.5, 0) rectangle (6*2 + 1.2/2, 1.5);

  \foreach \x/\label in {0/{$T-1$}, 6/{$T$}, 2*6/{$T+1$}} {
    \draw[very thick] (\x, -0.6) -- (\x, 1.5 + 0.6);
    \node[above, black] at (\x, 1.5 + 0.6) {\bfseries \label}; 
  }

\end{tikzpicture}
\caption{An example of two allocations, where there is enough room to borrow time for the larger one.}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\centering
\begin{tikzpicture}[>=Latex, font=\large, scale=0.5, transform shape]

  \foreach \x in {0, 6, 2*6} {
    \fill[gray!20] (\x - 1.2/2, -0.6) rectangle (\x + 1.2/2, 1.5 + 0.6);
  }

  \draw[thick, fill=white] (-.6, 0) rectangle (6 + 0.5, 1.5);
  
  \draw[thick, fill=white] (6 - 0.5, 0) rectangle (2*6 + 1.2/2, 1.5);

  \draw[pattern=north east lines, pattern color=black] (6 - 0.5, 0) rectangle (6 + 0.5, 1.5);

  \foreach \x/\label in {0/{$T-1$}, 6/{$T$}, 2*6/{$T+1$}} {
    \draw[very thick] (\x, -0.6) -- (\x, 1.5 + 0.6);
    \node[above, black] at (\x, 1.5 + 0.6) {\bfseries \label};
  }

\end{tikzpicture}
\caption{An example of two allocations, where there is not enough room as both require borrowing, causing an overlap.}
\end{minipage}

\end{figure}

\chapter{State of the Art Analysis}
\label{ch:sota}


The Mixed-Model Assembly Line (MMAL) problem is formally classified as NP-Hard. Traditionally, these optimization problems are addressed using heuristic or meta-heuristic algorithms. 

In recent years, Deep Reinforcement Learning (DRL) has become a dominant approach for solving dynamic scheduling problems. Unlike supervised methods, DRL agents learn through trial-and-error interaction with a simulation environment, aiming to maximize a cumulative reward signal.

For example, Chen et al. proposed an Adaptive Deep Q-Network, which builds on the Q-Learning Agorithm in order to address scheduling in Cloud Manufacturing environments characterized by random task arrivals. Their approach utilizes a resizable network structure to adapt to changing machine availability and employs a complex reward mechanism to balance multiple objectives, such as minimizing work time, and optimizing machine load. \cite{ref10} 

However, reinfocement learning requires trying and failing repeatedly in order to learn, and applying such trial and error Reinforcement Learning methods to a Mixed-Model Assembly Line (MMAL), especially in a production environment, will pose significant risks. In a cloud environment, a scheduling error typically results in a slowdown in the form of increased latency or reduced throughput. In contrast, a scheduling error in a physical MMAL situation often results in scheduling overlaps that force the entire assembly line to halt in order for each station to be able to finish in time. Given these higher stakes, the exploratory nature of reinforcement learning agents may be prohibitively costly compared to imitating provene human strategies, hence I've chosen to use Deep Unsupervised Learning instead.

While DRL is effective for dynamic environments where rules change frequently, it presents significant implementation challenges. It requires the construction of a high-fidelity simulation environment and the careful engineering of state and action spaces. Furthermore, the "black box" nature of the reward signal makes it difficult to capture the nuanced, unwritten knowledge of the human servicemen, which is the primary objective of this thesis.

Current manual approaches rely heavily on the "tacit knowledge" of management staff, knowledge accumulated through experience that is difficult to articulate as explicit rules. This thesis proposes capturing this knowledge using Supervised Learning, formally known in this context as Imitation Learning. 


\section{Recurrent Architectures}
\label{sec:recurrent_arch}

To emulate the sequential decision-making process of human schedulers, we first investigate architectures adapted from Natural Language Processing (NLP) that process data sequentially.

\subsection{Long Short-Term Memory (LSTM)}
\label{subsec:lstm}

In previous works addressing similar scheduling problems, researchers have applied Recurrent Neural Networks (RNNs), often utilizing Long Short-Term Memory (LSTM) units within a sequence-to-sequence (Seq2Seq) framework \cite{ref2}.

A defining characteristic of LSTMs is their ability to selectively forget irrelevant or outdated information via the "forget gate". This mechanism allows the model to focus on relevant patterns over time, mitigating the vanishing gradient problem inherent in standard RNNs and improving the modeling of long-term dependencies \cite{ref4, ref9}.

\subsection{Sequence-to-Sequence (Seq2Seq) Models}

Seq2Seq models, typically built upon encoder-decoder RNN architectures, are designed to map an input sequence to an output sequence of a different length or order \cite{ref2}.  While traditionally applied to machine translation (e.g., transforming English to Swedish), this architecture is adaptable to the assembly line problem. In this context, the input is a sequence of vectorized product orders, and the output is the permuted production sequence \cite{ref3}.

However, applying standard Seq2Seq models to permutation problems presents a specific challenge: \textbf{vocabulary definition}. Standard NLP models select outputs from a fixed, pre-defined vocabulary, similar to a dictionary. In a manufacturing context, where every day's product list is unique, a fixed vocabulary is insufficient. The model must instead learn to select from the dynamic input available that day, a task that is non-trivial for standard Seq2Seq architectures unless they strictly rely on abstract sequence indices \cite{ref2, ref3}.

One way to remedy this limitation is to modify the decoder to point directly to the input elements, an approach that leads to the development of Pointer Networks (discussed in Section \ref{subsec:pointer_nets}).

\subsection{Limitations of Recurrent Architectures}

Despite their historical success, RNN and LSTM-based models suffer from a significant bottleneck: \textbf{sequential processing}. This is because these models must process the input sequence step-by-step, so that item $t$ depends on item $t-1$, they cannot parallelize computation either. This results in slower training times and, more importantly, a limited ability to capture a lot of global context across the entire schedule simultaneously \cite{ref4}.

This limitation motivates the shift toward \textbf{Attention-based approaches}, which process the entire sequence in parallel, and has a larger global context scope.

\section{Attention-Based Architectures}
\label{sec:attention_arch}

\subsection{The Transformer Model}
\label{subsec:transformer}

To address the sequential bottlenecks of RNNs, Transformer-based architectures discard recurrence entirely, relying instead on a self-attention mechanism \cite{ref5}.

The defining characteristic of the Transformer is the use of scaled dot-product attention. This allows the model to weigh the importance of each element in a given sequence relative to all other elements in that same sequence, regardless of distance. This parallelized computation not only accelerates training but enables the model to capture global dependencies more effectively than RNN-based methods. Since Transformers are order-agnostic, positional encodings are added to the input vectors to retain sequence order information \cite{ref5}.

For scheduling problems, this translates into the ability to model complex interactions across the entire planning horizon. The placement of one item can be directly conditioned on all others in the same day's sequence, providing the context awareness necessary for effective assembly line scheduling \cite{ref8}.

\subsection{Pointer Networks}
\label{subsec:pointer_nets}

While Transformers excel at context, they still typically rely on generating tokens from a fixed vocabulary. The \textbf{Pointer Network} is the only architecture in this analysis specifically designed for combinatorial optimization and permutation problems \cite{ref7}.

Implemented using an encoder-decoder structure (often LSTM-based), the Pointer Network utilizes a specialized attention mechanism to generate context vectors that "point" to specific elements in the input sequence rather than predicting a value from a dictionary. This explicitly solves the fixed vocabulary problem: the model selects which station to place next directly from the available input pool. This significantly reduces the risk of repeating or hallucinating items, a common failure mode in standard Seq2Seq models \cite{ref7}.

\chapter{Methodology}
\section{The Heuristic Approach}

The problem to properly order manufacturing assembly lines with as few overlaps as possible is considered an NP-Hard problem, as it is an optimization problem.

\begin{figure}[ht]
    \centering

\begin{tikzpicture}[>=Latex, node distance=2cm, thick]

  \node[draw, minimum width=2cm, minimum height=1cm, align=center] (CH) {CH};
  \node[draw, minimum width=2cm, minimum height=1cm, align=center, right=of CH] (MH) {MH};

  \draw[->] ($(CH.west)+(-2,0)$) -- (CH.west) node[pos=0, above] {input};
  
  \draw[->] (CH.east) -- (MH.west) node[midway, above] {output};

  \draw[->] (MH.east) -- ++(1,0) -- ++(0,1) -- ++(-2,0) -- (MH.north);      

  \draw[->] (MH.east) -- ++(2,0) node[pos=1, below] {result};

\end{tikzpicture}
    \caption{Heuristic solution}
    \label{fig:solution}
\end{figure}

The Algorithm designed to solve this problem is a heuristic solution that will be made out of a Construction Hieuristic $(C\!H)$ that produces a starting point based on pre-defined constraints, that feeds into a Meta Heuristic $(M\!H)$ that finds a better solution starting from the output of the construction heuristic and self-improving until an acceptable result is returned. 


\section{Complementing the Heuristic Approach using Machine Learning}
Due to the fact that management workers today place the items manually using tacit knowledge that they have accumulated over the years, and in some cases can tell at first glance if a sequence may create problems, then what this thesis proposes is to emulate that exact same knowledge by learning which placement patterns tend to work together and which do not.

The idea is that; if these workers have knowledge of an adequate solution from the get-go with some risk of overlap, then we can train a Deep Learning Model $(M\!L)$ on such previous data to give the algorithm a better starting point, thus (in theory) reducing the runtime of that algorithm.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[>=Latex, node distance=2cm, thick]
        \node[draw, minimum width=2cm, minimum height=1cm, align=center] (MH) {MH};
\node[draw, minimum width=2cm, minimum height=1cm, below left=1cm and 2cm of MH] (ML) {ML};
        \node[draw, align=center, right=of ML] (Data) {Data};
        \node[draw, dotted, minimum width=2cm, minimum height=1cm, align=center, left=4cm of MH] (CH) {CH};
        \draw[->] (MH.east) -- ++(1,0) -- ++(0,1) -- ++(-2,0) -- (MH.north);      

        \draw[->] (MH.east) -- ++(2,0) node[pos=1, below] {result};

        \draw[->] ($(ML.west)+(-2,0)$) -- (ML.west) node[pos=0, above] {input};

        \draw[->] (Data.west) -- (ML.east);

        \draw[->] (ML.north) |- (MH.west);

    \end{tikzpicture}
    \caption{ML solution}
    \label{fig:ml_solution}
\end{figure}

However it is worth to consider that such an approach can prove redundant or yield worse results if the problem at hand is an "easy" problem where many solutions can be found quickly, as opposed to a "hard" problem where a desired solution may not even be found. 

\section{JSON to Vector Methodology}
Machine learning models operate on numerical vector data, often represented as \textit{tensors}, rather than on raw JSON structures. Given a list of JSON objects on an assembly line, each JSON can be encoded into a fixed-length vector $x_t$ by extracting and normalizing its features. This process transforms the entire list into a sequence of vectors ${[x_1, x_2, \dots, x_N]}$. Using sequences from historical data, a model can then be trained to learn a mapping from an input order of JSONs to a desired output order.

\section{Transforming a language-based model into a sequence based model}

The models examined in this thesis were originally designed for natural language processing tasks, such as translation between languages. Both sequence-to-sequence and transformer-based architectures rely on a predefined vocabulary. This raises the question of whether a list of products can be treated analogously to a collection of words where each product variation forms a distinct element within the vocabulary. Doing so could allow the model to more accurately replicate historical data, along with its flaws.

However, certain constraints must be imposed as language models often overproduce common tokens such as “the,” since these words are statistically more likely to appear in most positions in any given English sentence. To avoid this bias, the model must instead operate on genuine rearrangements or permutations relevant to our specific application.

\subsection{Workarounds}
\label{chap:workarounds}

To adapt the Transformer architecture for product sequencing without suffering from the previously mentioned limitations, two specific architectural modifications were implemented:

\subsubsection{Continuous-Discrete Embedding}
Standard NLP models map a finite set of words to static vectors. Since product variations are continuous rather than discrete, a traditional vocabulary approach would require an infinite vocabulary size to capture every possible variation.

To resolve this, the implementation utilizes a custom \textit{ObjectEmbedding} layer. The discrete station identifier is mapped via a standard lookup table, while the size value is projected through a linear layer. These two feature sets are concatenated to form the input vector:
\begin{equation}
E_{input} = \text{Concat}(\text{Embedding}(ID), \text{Linear}(Size)) %TODO: dubbelkolla
\end{equation}
This allows the attention mechanism to process the semantic identity of the station, enabling the model to generalize to unseen product sizes without expanding the vocabulary.

\subsubsection{Decoupling Prediction from Sequencing}
\label{subp:decoupling}
To strictly prevent the common issue of repeting tokens in generative language models, and instead repurpose it for permutation, the sequence generation decoupled from the neural network entirely.

The Transformer is configured as a standalone Encoder, serving purely as a regressor to predict the offsets of each station. It does not output a probability distribution over the next token. Instead, the final reordering is performed by a combinatorial optimizer (Simulated Annealing) which utilizes the Transformer's predictions to minimize a conflict cost matrix:
\begin{equation}
\text{Cost}_{ij} = \sum (\max(0, | \text{Drift}_i | + | \text{Drift}_j | - \text{Limit}))^2 %TODO: dubbelkolla
\end{equation}

By using the Transformer solely for feature extraction, and delegating the sorting logic to mathematics, the system ensures that the output is always a valid permutation of the input, which will guarantuee that no items become duplicated or omitted.


\chapter{System Architecture}
\section {Data generation}
\subsection{Constants}
For data generation the following constants were used:
\begin{table}[htbp]
    \centering
    \caption{Table of Constants used in Data Generation}
    \label{tab:generation_constants}
    \begin{tabular}{l c l} % Left-aligned, Centered, Left-aligned
        \toprule
        \textbf{Constant} & \textbf{Default Value} & \textbf{Description} \\ 
        \midrule
        Objects    & 100     & Total number of objects in the testing sequence \\
        Stations   & 35      & Number of stations \\
        Takt       & 700     & The cycle time ($Cmin$) \\
        Drift      & 200     & The size of the drift area ($Cmin$) \\
        Gap        & 10       & Minimum buffer distance between objects \\
        Multiplier & 500     & Constant value to multiply \textit{Objects} with \\
        \bottomrule
    \end{tabular}
\end{table}
\subsection{Data Preprocessing and Abstraction}
For the purpose of training optimization, the raw information is abstracted into a concise JSON schema. Each entry in the dataset is reduced to the following three attributes:

\begin{itemize}
    \item \textbf{Object ID} (\texttt{object}): An integer representing the unique identifier of the item.
    \item \textbf{Station Data} (\texttt{data}): A mapping of station keys ($s_1, \dots, s_{35}$) to their respective time requirement values.
    \item \textbf{Station Offsets} (\texttt{offsets}): A mapping of station keys to their relative offset values.
\end{itemize}

By utilizing these values, we can train a model to prioritize time-based decision-making over rigid rule-based systems. It is important to note that while the model operates without explicit constraints, the use of a sufficiently large dataset derived from expert tacit knowledge ensures that it implicitly adheres to fundamental ground rules, albeit with greater flexibility.

\subsection{Offset Calculation and Centering}
The positioning of an object within a station is determined by its \textit{offset} relative to the \textbf{Takt} start ($0$ locally). The script employs a dynamic centering logic that positions tasks within the remaining available capacity of the standard Takt window. This approach balances the empty space (or buffer) on either side of the allocation, adapting to the finish time of the preceding task.

To determine the position, the script first calculates the local start time, $t_{start}$, which corresponds to the completion of the previous allocation relative to the current Takt. The remaining buffer, $B$, is calculated as the difference between the standard Takt duration $T$ and the occupied time, clamped to zero to ensure non-negative spacing:

\begin{equation}
    B = \max(0, T - t_{start} - S)
\end{equation}

Where $S$ is the size (duration) of the new task. The tentative offset, $O_{calc}$, is then derived by placing the task at the start position plus half of the calculated buffer:

\begin{equation}
    O_{calc} = t_{start} + \frac{B}{2}
\end{equation}

This formula ensures that if the previous task finishes early, the new task is centered in the wide gap; if the previous task finishes late, the buffer shrinks, effectively pulling the new task closer to the previous one.

Finally, to ensure the object adheres to physical station boundaries and drift limitations ($D$), the calculated offset is clamped. The maximum permissible offset is defined such that the task end does not exceed the drift limit ($T+D$):

\begin{equation}
    O_{calc} = \max(-D, \min(O_{calc}, T + D - S))
    \label{eq:offset_constraint}
\end{equation}

\begin{figure}[htbp]
\centering

\begin{minipage}{0.48\textwidth}
\centering
\begin{tikzpicture}[>=Latex, scale=0.85, transform shape]

  \fill[gray!20] (-1.2, -0.5) rectangle (0, 2);

  \fill[gray!20] (5, -0.5) rectangle (6.2, 2);

  \draw[draw, fill=white] (-0.3,0) rectangle (4.5, 1.5) node[pos=.5] {Allocation};

  \draw[thick] (-0.6, -0.5) -- (-0.6, 2);
  \draw[thick] (5.6, -0.5) -- (5.6, 2);

  \node[above] at (-0.6, 2) {$T$};
  \node[above] at (5.6, 2) {$T+1$};

  \draw[|-|] (-0.3, 1.8) -- (4.5, 1.8) node[midway, above] {$S$};

  \draw[|-|] (-0.6, -0.7) -- (-0.3, -0.7) node[midway, below] {$O$};

  \draw[|-|] (4.5, -0.7) -- (6.2, -0.7) node[midway, below] {$O + T - S$};

\end{tikzpicture}
    \caption{Allocation positioning before the centering process (FIXME)}
\end{minipage}\hfill
\begin{minipage}{0.48\textwidth}
\centering
\begin{tikzpicture}[>=Latex, scale=0.85, transform shape]

  \fill[gray!20] (-1.2, -0.5) rectangle (0, 2);

  \fill[gray!20] (5, -0.5) rectangle (6.2, 2);

  \draw[draw, fill=white] (0.1,0) rectangle (4.9, 1.5) node[pos=.5] {Allocation};

  \draw[thick] (-0.6, -0.5) -- (-0.6, 2);
  \draw[thick] (5.6, -0.5) -- (5.6, 2);

  \node[above] at (-0.6, 2) {$T$};
  \node[above] at (5.6, 2) {$T+1$};

  \draw[|-|] (-0.6, -0.7) -- (0.1, -0.7) node[midway, below] {$O_{calc}$};

\end{tikzpicture}
\caption{Allocation after centering.}
\end{minipage}

\end{figure}

\subsection{Constructive Data Generation as a Solved Puzzle}

To train the machine learning model, a synthetic dataset is required that represents valid, collision-free production schedules. Rather than randomly placing tasks and checking for validity, the system employs a constructive generation algorithm. This approach acts as a forward simulation, placing allocations sequentially to inherently create a "solved puzzle" which serves as the ground truth for training.

\subsubsection*{Dependency-Based Positioning}
The placement of any given allocation $A_{i,j}$ (Object $i$ at Station $j$) is not independent. It is strictly constrained by two spatial-temporal boundaries derived from the existing grid state:
\begin{enumerate}
    \item \textbf{Station Availability:} The station $j$ must be free. Therefore, $A_{i,j}$ cannot start until the previous object $i-1$ has departed station $j$.
    \item \textbf{Object Availability:} The object $i$ must be ready. Therefore, $A_{i,j}$ cannot start until object $i$ has finished processing at the previous station $j-1$.
\end{enumerate}

Mathematically, the earliest valid start time ($t_{start}$) for the new allocation is defined as the maximum of these two completion timestamps, plus a configured safety gap:

\begin{equation}
    t_{start} = \max(End_{i-1, j}, \quad End_{i, j-1}) + Gap
\end{equation}

This logic ensures that the generated schedule is causally valid—no object effectively "teleports" between stations, and no two objects overlap on a single station.

\subsubsection*{Context-Aware Scaling}
The dimensions of the tasks are also generated dependently. While a desired duration is chosen stochastically, the actual assigned size ($S$) is clamped by the remaining available time in the Takt window. The generator calculates the available space between the calculated $t_{start}$ and the maximum drift boundary. If the random size exceeds this space, it is truncated to fit.

This process results in a dataset where every input (the sequence of objects) has a perfectly matching output (the offsets and sizes) that is guaranteed to be solvable, providing a robust target for supervised learning.

(FIXME)

\begin{figure}[H] 
\centering
\begin{minipage}{0.48\textwidth}
    \centering
    \begin{tikzpicture}[>=Latex, font=\small, scale=0.8, transform shape]

        \fill[pattern=north east lines, pattern color=gray] (-2.5, 2.0) rectangle (2.5, 3.5);
        \fill[pattern=north east lines, pattern color=gray] (-2.5, 0) rectangle (2.5, 1.5);
        \draw[gray!50] (-2.5, 2.0) -- (6, 2.0);
        \draw[gray!50] (-2.5, 3.5) -- (6, 3.5);
          
        \draw[gray!50] (-2.5, 0) -- (6, 0);
        \draw[gray!50] (-2.5, 1.5) -- (6, 1.5);

        % Added Station Labels
        \node[right] at (6, 2.75) {$S_{n-1}$};
        \node[right] at (6, 0.75) {$S_{n}$};

        \draw[thick] (0, -0.2) -- (0, 3.7);
        \node[above] at (0, 3.7) {$T$};
        \draw[fill=white, draw=black] (-2.2, 2.2) rectangle (1.5, 3.3);

        \draw[thick, dashed, darkgray] (1.5, 2.2) -- (1.5, -0.6);
        \node[below] at (1.5, -0.6) {$t_{1}$};

        \draw[fill=gray!60, draw=black] (-2.2, 0.2) rectangle (0.5, 1.3);

        \draw[thick, dashed, darkgray] (0.5, 0.2) -- (0.5, -0.6);
        \node[below] at (0.5, -0.6) {$t_{2}$};
         
        \draw[fill=white, draw=black, thick] (1.8, 0.2) rectangle (5.5, 1.3);
        \fill[white] (5.2, 0.22) rectangle (5.5, 1.28);
         
        \node[font=\bfseries] at (3.65, 0.75) {};

        \draw[->, thick, black] (1.5, 0.75) -- (1.8, 0.75);
        \draw[dotted, thick] (1.8, 0) -- (1.8, 1.5);
        \node[below] at (2.1, -0.1) {$t_{start}$};

    \end{tikzpicture}
    \caption{Case 1}
\end{minipage}\hfill
\begin{minipage}{0.48\textwidth}
    \centering
    \begin{tikzpicture}[>=Latex, font=\small, scale=0.8, transform shape]

        \fill[pattern=north east lines, pattern color=gray] (-2.5, 2.0) rectangle (2.5, 3.5);
        \fill[pattern=north east lines, pattern color=gray] (-2.5, 0) rectangle (2.5, 1.5);
        \draw[gray!50] (-2.5, 2.0) -- (6, 2.0);
        \draw[gray!50] (-2.5, 3.5) -- (6, 3.5);
          
        \draw[gray!50] (-2.5, 0) -- (6, 0);
        \draw[gray!50] (-2.5, 1.5) -- (6, 1.5);

        % Added Station Labels
        \node[right] at (6, 2.75) {$S_{n-1}$};
        \node[right] at (6, 0.75) {$S_{n}$};

        \draw[thick] (0, -0.2) -- (0, 3.7);
        \node[above] at (0, 3.7) {$T$};
        \draw[fill=white, draw=black] (-2.2, 2.2) rectangle (0.5, 3.3);


        \draw[fill=gray!60, draw=black] (-2.2, 0.2) rectangle (1.6, 1.3);
          
        \draw[fill=white, draw=black, thick] (1.8, 0.2) rectangle (5.5, 1.3);
        \fill[white] (5.2, 0.22) rectangle (5.5, 1.28);
          
        \node[font=\bfseries] at (3.65, 0.75) {};

        \draw[thick, dashed, darkgray] (.5, 2.2) -- (.5, -0.6);
        \node[below] at (.5, -0.6) {$t_{1}$};

        \draw[thick, dashed, darkgray] (1.6, 0.2) -- (1.6, -0.6);
        \node[below] at (1.6, -0.6) {$t_{2}$};

        \draw[->, thick, black] (1.5, 0.75) -- (1.8, 0.75);
        \draw[dotted, thick] (1.8, 0) -- (1.8, 1.5);
        \node[below] at (2.1, -0.1) {$t_{start}$};

    \end{tikzpicture}
    \caption{Case 2}
\end{minipage}
\end{figure}

\section{Machine Learning Models}

\begin{table}[htbp]
    \centering
    \caption{Table of Constants used for Machine Learning (Excluding paths)}
    \label{tab:generation_constants}
    \begin{tabular}{l c} % Left-aligned, Centered, Left-aligned
        \toprule
        \textbf{Constant} & \textbf{Default Value} \\ 
        \midrule
        batch\_size &  $16$ \\
        num\_epochs & $10$ \\
        lr & $1e^{-4}$ \\
        d\_model & $512$ \\
        dropout & $0.1$ \\
        \bottomrule
    \end{tabular}
\end{table}
\subsection{The Transformer Model}
\label{sec:trans}

The architecture utilized in this thesis is heavily based on the Transformer model proposed by Vaswani et al. \cite{ref5}, with modifications tailored specifically to the Mixed-Model Assembly Line (MMAL) problem, and adapted to function as a cost estimator for the assembly line. For one, the decoder has been decoupled to yield better results (see \ref{subp:decoupling}), and instead of processing text, the model has been fitted with a custom embedding layer which is tasked with reading the size of the work-load within a station, and find a better fit. (FIXME)

\subsubsection{Simulated Annealing} While the Transformer predicts the local impact of individual items, the global sequence scheduling is addressed using Simulated Annealing. This method explores the solution space of item permutations, using a cost matrix derived from the Transformer's drift predictions. By allowing for probabilistic acceptance of sub-optimal transitions Simulated Annealing avoids local optima that typically constrain greedy algorithms, resulting in a more stable assembly line schedule. (FIXME)


\subsubsection{Unhooking the decoder}
While testing the feasibility of the transformer model, I found that it yielded better results by switching to a BERT-like model rather than a Sequence-to-Sequence based approach. (FIXME)


\subsection{The Pointer Network Model}
The proposed solution implements a Pointer Network architecture using the PyTorch framework. The model essentially follows a standard Encoder-Decoder structure enhanced with a content-based attention mechanism, designed to handle variable-length input sequences and output a permutation of the input indices. (FIXME)

\subsubsection{Encoder Mechanism}
The encoder processes the input sequence to generate a representation of the initial problem state.
\begin{enumerate}
    \item \textbf{Input Projection:} Raw input features are first passed through a projection block consisting of a linear layer, a $tanh$ function, and Layer Normalization. This maps the input dimension to a higher-dimensional latent space ($d_{model}$).

    \item \textbf{Bidirectional LSTM:} The projected features are fed into a Bi-directional LSTM. This allows the model to capture context from both past and future states in any given sequence. The final hidden states of the forward and backward passes are combined to form the initial context for the decoder.
\end{enumerate}

\subsubsection{Decoder and Attention Mechanism}
The decoder generates the output sequence one item at a time. Unlike standard NLP models that generate new tokens from a fixed vocabulary, this decoder selects indices from the input sequence.
\begin{itemize}
    \item \textbf{State Initialization:} The decoder's initial hidden and cell states are initialized by projecting the final concatenated states of the encoder through learnable linear "bridge" layers.
    \item \textbf{Glimpse Mechanism:} To improve the the attention mechanism, the model employs a "glimpse" step. At every time step, the decoder's current state queries the encoder outputs to calculate a context vector. This vector summarizes the most relevant parts of the input sequence for the current step.
    \item \textbf{Pointer Selection:} The final selection is performed by a second attention layer. This layer combines the decoder's current state with the context vector from the glimpse step to compute a probability distribution over the input elements. The element with the highest probability is selected as the next index in the sequence.
\end{itemize}

\subsubsection{Masking and Inference}
To ensure the validity of the resulting permutation, a masking mechanism is applied during inference. Once an input index is selected, it is masked out by setting its log-probability to $-\infty$, this will prevent the model from selecting the same element twice. The selected element's embedding is then used as the input for the next decoder time step.

\subsubsection{The Dataset}
To enable the Pointer Network to process the production schedules, the raw JSON data is transformed into structured numerical tensors through a custom preprocessing pipeline. This pipeline is responsible for extracting the relevant features from each object, normalizing the station processing times, and organizing the sequences into batches for efficient model training. (FIXME)
\begin{itemize}
    \item \textbf{Tensor Representation} \\
    Each input sequence is converted into a numerical tensor of shape $(L, N)$, where $L$ denotes the sequence length (number of objects) and $N$ corresponds to the normalized processing times for each station ($S_1$, \dots, $S_N$). For batch processing on the GPU, these tensors are padded to a uniform length $L_{max}$ and stacked, resulting in a final input structure of dimensions $(B, L_{max}, N)$, where $B$ represents the batch size.

    \item \textbf{Propagation and Usage} \\
    Upon entering the Pointer Network, this raw feature tensor is first processed by the \textit{Projector} module. A linear layer expands the input dimension $S$ into a higher-dimensional latent representation ($d_{model}$). This projected embedding is subsequently passed through a \texttt{Tanh} activation and \texttt{LayerNorm} before being consumed by the Bidirectional LSTM encoder, which generates the context-aware hidden states required for the attention mechanism.
\end{itemize}

\subsection{The Training Loop}
(FIXME), (see \ref{chap:workarounds})
\subsection{The Configuration}

\subsection{The Application Programming Interface (API)}
The system exposes its core functionalities through a RESTful API, facilitating programmatic interaction with the implemented neural architectures. This interface serves as the primary gateway for data ingestion and inference execution. (FIXME)

\subsection{Endpoints}

\subsubsection{/run/\{model\_type\}}

This POST endpoint provides a unified interface for sequence processing across different model architectures (\textbf{Transformer}, \textbf{Pointer Network}, or \textbf{Seq2Seq}). Once selected it returns a processed sequence made from the input data, including refitting. 

\subsubsection{/run/\{model\_type\}/$int:n$/}

This POST endpoint is the same as the one above it, but it accepts an integer $n$, which runs the model $n$ times and returns $n$ suggested sequences. (FIXME)


\subsubsection{/check/}

This POST endpoint will count the number of \textbf{overlaps} on a given sequence, based on configuration data. 


\chapter{Experiments and results}
\section{Task Performance}
Performance is measured by how well the model can take an input with overlapping data and rearrange the tasks to minimize overlaps. The reduction metric is defined at \ref{sec:eval}.

Although the operational constraints of the target problem typically limit sequences to batches of roughly 100 objects, the following results also evaluate the model's scalability beyond this range.

\subsection{Pointer Network Performance}

\begin{table}[H]
    \centering
    \small 

    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \caption{Test-size of $10$}
        \label{tab:res_10}
        \begin{tabular}{l c c}
            \toprule
            \textbf{Run} & \textbf{Ratio ($R$)} & \textbf{Red.} \\
            \midrule
            Run \#1 & $1/25$ & $96\%$ \\
            Run \#2 & $0/34$ & $100\%$ \\
            Run \#3 & $2/20$ & $90\%$ \\
            \midrule
            \textbf{Avg} & & $95.3\%$ \\
            \bottomrule
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \caption{Test-size of $100$}
        \label{tab:res_100}
        \begin{tabular}{l c c}
            \toprule
            \textbf{Run} & \textbf{Ratio ($R$)} & \textbf{Red.} \\
            \midrule
            Run \#1 & $83/367$ & $77.4\%$ \\
            Run \#2 & $75/346$ & $78.3\%$ \\
            Run \#3 & $64/340$ & $81.2\%$ \\
            \midrule
            \textbf{Avg} & & $79.0\%$ \\
            \bottomrule
        \end{tabular}
    \end{minipage}

    \vspace{0.8cm} 

    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \caption{Test-size of $1000$ (Run B)}
        \label{tab:res_100b}
        \begin{tabular}{l c c}
            \toprule
            \textbf{Run} & \textbf{Ratio ($R$)} & \textbf{Red.} \\
            \midrule
            Run \#1 & $83/367$ & $77.4\%$ \\
            Run \#2 & $75/346$ & $78.3\%$ \\
            Run \#3 & $64/340$ & $81.2\%$ \\
            \midrule
            \textbf{Avg} & & $79.0\%$ \\
            \bottomrule
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \caption{Test-size of $10000$ (Run C)}
        \label{tab:res_100c}
        \begin{tabular}{l c c}
            \toprule
            \textbf{Run} & \textbf{Ratio ($R$)} & \textbf{Red.} \\
            \midrule
            Run \#1 & $83/367$ & $77.4\%$ \\
            Run \#2 & $75/346$ & $78.3\%$ \\
            Run \#3 & $64/340$ & $81.2\%$ \\
            \midrule
            \textbf{Avg} & & $79.0\%$ \\
            \bottomrule
        \end{tabular}
    \end{minipage}
\end{table}


\subsection{Transformer Performance}

\begin{table}[H]
    \centering
    \small 

    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \caption{Test-size of $10$ (FIXME)}
        \label{tab:res_10}
        \begin{tabular}{l c c}
            \toprule
            \textbf{Run} & \textbf{Ratio ($R$)} & \textbf{Red.} \\
            \midrule
            Run \#1 & $1/25$ & $96\%$ \\
            Run \#2 & $0/34$ & $100\%$ \\
            Run \#3 & $2/20$ & $90\%$ \\
            \midrule
            \textbf{Avg} & & $95.3\%$ \\
            \bottomrule
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \caption{Test-size of $100$ (FIXME)}
        \label{tab:res_100}
        \begin{tabular}{l c c}
            \toprule
            \textbf{Run} & \textbf{Ratio ($R$)} & \textbf{Red.} \\
            \midrule
            Run \#1 & $83/367$ & $77.4\%$ \\
            Run \#2 & $75/346$ & $78.3\%$ \\
            Run \#3 & $64/340$ & $81.2\%$ \\
            \midrule
            \textbf{Avg} & & $79.0\%$ \\
            \bottomrule
        \end{tabular}
    \end{minipage}

    \vspace{0.8cm} 

    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \caption{Test-size of $1000$ (Run B) (FIXME)}
        \label{tab:res_100b}
        \begin{tabular}{l c c}
            \toprule
            \textbf{Run} & \textbf{Ratio ($R$)} & \textbf{Red.} \\
            \midrule
            Run \#1 & $83/367$ & $77.4\%$ \\
            Run \#2 & $75/346$ & $78.3\%$ \\
            Run \#3 & $64/340$ & $81.2\%$ \\
            \midrule
            \textbf{Avg} & & $79.0\%$ \\
            \bottomrule
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \caption{Test-size of $10000$ (Run C) (FIXME)}
        \label{tab:res_100c}
        \begin{tabular}{l c c}
            \toprule
            \textbf{Run} & \textbf{Ratio ($R$)} & \textbf{Red.} \\
            \midrule
            Run \#1 & $83/367$ & $77.4\%$ \\
            Run \#2 & $75/346$ & $78.3\%$ \\
            Run \#3 & $64/340$ & $81.2\%$ \\
            \midrule
            \textbf{Avg} & & $79.0\%$ \\
            \bottomrule
        \end{tabular}
    \end{minipage}
\end{table}


\subsection{Seq2Seq performance}

\begin{table}[H]
    \centering
    \small 

    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \caption{Test-size of $10$ (FIXME)}
        \label{tab:res_10}
        \begin{tabular}{l c c}
            \toprule
            \textbf{Run} & \textbf{Ratio ($R$)} & \textbf{Red.} \\
            \midrule
            Run \#1 & $1/25$ & $96\%$ \\
            Run \#2 & $0/34$ & $100\%$ \\
            Run \#3 & $2/20$ & $90\%$ \\
            \midrule
            \textbf{Avg} & & $95.3\%$ \\
            \bottomrule
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \caption{Test-size of $100$ (FIXME)}
        \label{tab:res_100}
        \begin{tabular}{l c c}
            \toprule
            \textbf{Run} & \textbf{Ratio ($R$)} & \textbf{Red.} \\
            \midrule
            Run \#1 & $83/367$ & $77.4\%$ \\
            Run \#2 & $75/346$ & $78.3\%$ \\
            Run \#3 & $64/340$ & $81.2\%$ \\
            \midrule
            \textbf{Avg} & & $79.0\%$ \\
            \bottomrule
        \end{tabular}
    \end{minipage}

    \vspace{0.8cm} 

    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \caption{Test-size of $1000$ (Run B) (FIXME)}
        \label{tab:res_100b}
        \begin{tabular}{l c c}
            \toprule
            \textbf{Run} & \textbf{Ratio ($R$)} & \textbf{Red.} \\
            \midrule
            Run \#1 & $83/367$ & $77.4\%$ \\
            Run \#2 & $75/346$ & $78.3\%$ \\
            Run \#3 & $64/340$ & $81.2\%$ \\
            \midrule
            \textbf{Avg} & & $79.0\%$ \\
            \bottomrule
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \caption{Test-size of $10000$ (Run C) (FIXME)}
        \label{tab:res_100c}
        \begin{tabular}{l c c}
            \toprule
            \textbf{Run} & \textbf{Ratio ($R$)} & \textbf{Red.} \\
            \midrule
            Run \#1 & $83/367$ & $77.4\%$ \\
            Run \#2 & $75/346$ & $78.3\%$ \\
            Run \#3 & $64/340$ & $81.2\%$ \\
            \midrule
            \textbf{Avg} & & $79.0\%$ \\
            \bottomrule
        \end{tabular}
    \end{minipage}
\end{table}


\section{Dataset}
We obtained an official dataset from a production run. However, the raw data contained several inconsistencies that required resolution before it could be used for training. Hence a dummy dataset was generated for simulating this task, where borrowing and tighter fits are more commonplace than actual production data. (At least from what we saw from it) In theory this would be beneficial as the model is trained on "harder" sequences.


\section{Evaluation Metrics}
\label{sec:eval}
To objectively measure the performance of the model, we monitor performance of the models in overlap reduction (see \ref{chap:overlap}) using a simple ratio.
\begin{equation}
    \text{ratio} = \frac{O_{predicted}}{O_{source}}
\end{equation}
where $O_{source}$ is the total number of overlaps of the shuffled input sequence, and $O_{predicted}$ is the total number of overlaps of the predicted sequence. The closer to $0$ it gets, the better it performs, and if it goes above $1$ we're yielding worse results than before.


\section{Model tweaks}
\subsection{Pointer Network}
\subsubsection{Adding the offset handler}
    While the Transformer model used a simulated annealing algorithm for adjusting offsets on runtime, the same idea was applied to the pointer network (See \ref{sec:trans}) (FIXME)

\chapter{Discussion}
\section{Interpretation of Results}

    The results show that the implemented models performed within expectations, with the Pointer Network performing the best for our permutation-based task.

    This difference in performance highlights why choosing the right architecture matters. While a Transformer could theoretically solve the task given enough data and processing power, the Pointer Network proved to be a better fit for this optimization problem. This is in part because the Transformer was originally designed for Natural Language Processing \cite{ref5}, whereas the Pointer Network was specifically introduced to handle combinatorial problems where the output is a permutation of the input—most notably the Traveling Salesman Problem. \cite{ref7}


While the Transformer model could potentially yield improved results through extensive optimization and a larger training set, the computational cost may not be worth the marginal gains. Given that the core objective of this thesis essentially was permutational reordering of fixed time slots, then the Pointer Network is more theoretically aligned with the problem space. The Transformer's generative versatility offers no distinct advantage in this specific context, particularly as the Pointer Network similarly utilizes an attention mechanism to effectively solve these combinatorial constraints.


\section{Limitations}
\subsection{Hardware and Resource Limitations}
    While training the models on a larger set of data (as Transformers generally perform better with more training data) I ran out of available memory so the size of the training data had to be restricted to 500,000 objects at most, while Machine Learning models should in actuality contain millions of training objects. A reason for this could be how the data itself is parsed, and the bottleneck could lie in there. (FIXME)

    Even though Transformer-based architectures generally demonstrate improved generalization with larger datasets, the training size in this study had to be restricted to $500,000$ due to hardware memory constraints. This could have greatly impaced the performance, and feasibility of the Transformer model in this thesis. Preliminary analysis suggests that this bottleneck may stem from the current data parsing implementation, which could be optimized in future work to allow for larger batch sizes and more extensive training sets.

\section{Future Work}
    Future iterations of this research should prioritize larger datasets and utilize high performance computing clusters to overcome the memory bottlenecks encountered in this study. (FIXME)


\begin{thebibliography}{99}
\bibitem{ref2}
A. Dupuis, C. Dadouchi, and B. Agard, 
        \textit{A decision support system for sequencing production in the manufacturing industry},
\textit{Computers \& Industrial Engineering}, vol.~185, p.~109686, 2023. 
\bibitem{ref3} J. Lindén, 
    \textit{Understand and Utilise Unformatted Text Documents by Natural Language Processing Algorithms}, 
    Master’s thesis, Mid Sweden University, Department of Information and Communication Systems (IST), Spring 2017.
\bibitem{ref4} I. Pointer,
    \textit{Programming PyTorch for Deep Learning, ISBN: 9781492045359},
O'Reilly Media, 2019.
\bibitem{ref5} 
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, 
A. N. Gomez, Ł. Kaiser, and I. Polosukhin, 
\textit{Attention is All You Need}, Advances in Neural Information Processing Systems (NeurIPS), vol.~30, 2017.
\bibitem{ref6} 
C. Fink, O. Schelén, and U. Bodin, 
\textit{Work in progress: Decision support system for rescheduling blocked orders}, 
Department of Computer Science, Electrical and Space Engineering, Luleå University of Technology, 2023.
\bibitem{ref7} O. Vinyals, M. Fortunato, and N. Jaitly, 
    \textit{Pointer Networks}, 
    Department of Mathematics, University of California Berkeley, 2015.
\bibitem{ref8} E. Stevens, L. Antiga, T. Ciehmann,
    \textit{Deep Learning with PyTorch, ISBN: 9781617295263}, Manning Publications, 2020.
\bibitem{ref9} S. Hochreiter, \textit{The vanishing gradient problem during learning recurrent neural nets and problem solutions}, Institut für Informatik, Technische Universität München, D-80290, 1998.
\bibitem{refx} Author, Title, Journal, Year.
\end{thebibliography}
\end{document}


%TODO: Fler modeller, typ 2st till, Transformer, Seq2Seq, PointerNetwork
